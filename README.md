<div align="center">

# COCO LLM: A Large Language Model for Mainframe Modernization
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)&nbsp;
[![arXiv](https://img.shields.io/badge/2406.11927-red?style=flat&label=arXiv)](link)&nbsp;
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)



# Introduction

We are introducing **COCO LLM**, a state-of-the-art large language model (LLM) specifically designed with knowledge of mainframe legacy systems and COBOL codebases. COCO LLM is built on top of DeepSeek-Coder 7B and is available with 7B and 10.5B parameters. Additionally, we have created COCO bench, a comprehensive benchmark for assessing mainframe knowledge, including multiple-choice questions, question answering, and COBOL code summarization. Our empirical evaluations demonstrate that COCO LLM consistently outperforms existing state-of-the-art LLMs across these tasks. Specifically, COCO LLM achieves 30% higher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the BLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times higher than GPT-3.5 on COBOL summarization. Our work highlights the potential of COCO LLM to drive significant advancements in managing and modernizing legacy systems, thereby enhancing productivity and saving time for software developers.
